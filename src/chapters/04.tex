\chapter{Building and securing a company-level, production-grade infrastructure}
\label{chapter:infrastructure}

This chapter describes my approach of building and securing a production-grade infrastructure supporting the development, testing, and public operation of Diplomatiq.

\section{Introduction}

The underlying infrastructure plays a foundational role in the eventual success or failure of every business. 21\textsuperscript{st}-century companies often build their business operations entirely on information technology solutions, meaning a well-founded IT infrastructure is key to succeed. Even though at the time of writing this thesis, Diplomatiq is a company existing only in the future, the infrastructure I elaborate in the present will be the foundation of its business operation. By setting up a robust and secure infrastructure, I want to establish the future of Diplomatiq. It needs to be done right, so it does not need to be done again.

This involves two principles. The first is that key infrastructure elements need to be established using mature and robust solutions, so they do not need to be rebuilt or replaced later. This excludes trial versions of services, expiring student offers, and generally free solutions as well. Organizational hierarchy needs to be set up properly, allowing later expansion, and the infrastructure with all its access credentials should be documented meticulously. The second principle is that security should be taken into consideration from the very beginning. Authentication and authorization policies should be the as strict as possible, and all access credentials should be stored in a safely encrypted manner.

Throughout this chapter, I introduce the various infrastructure elements I evaluated, purchased and integrated into Diplomatiq's infrastructure. Security-related aspects will appear in most of the sections.

\section{Naming}
\label{section:naming}

A good brand name identifies a company in various ways. Apart from marketing purposes, the name should be suifficently unique to be usable within the various services and namespaces on the Internet. For this purpose, \emph{Diplomatiq} seemed to be suitable: it is unique, appropriately short for both domain names~\cite{howtochoosedomainname} and the human memory~\cite{memoryfour} with its 10~characters, and it characterizes its subject well.

I formulated the following — prioritized — guidelines for reserving namespaces for Diplomatiq across services on the Internet:

\begin{enumerate}
\item If available, use \emph{Diplomatiq} (capitalized).
\item Else if the service allows lowercase letters only, use \emph{diplomatiq}.
\item Else if use \emph{DiplomatiqOrg} (capitalized).
\item Else if use \emph{diplomatiqorg}.
\item Else use a custom name.
\end{enumerate}

As of now, there has been no need to apply the 5\textsuperscript{th} rule.

\section{Brand}

For visual recognition, a company needs a well-defined image. Diplomatiq's corporate identity was designed by one of my acquaintances, Roland Hidvégi. It includes a logotype, three variants of application icons, ten brand colors, and Eina~\cite{eina} as the advised font family\footnote{As Eina is not available as a free web font~\cite{eina-licensing}, I temporarily use Helvetica instead.}. \Cref{fig:diplomatiq-logotype} and \Cref{fig:diplomatiq-app-icons} shows the logotype and the application icon variants.

\begin{figure}[!htb]
    \centering
    \vspace{2mm}
    \includegraphics[width=8cm]{figures/diplomatiq-logo.pdf}
    \caption{The logotype of Diplomatiq}
    \label{fig:diplomatiq-logotype}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=8cm]{figures/diplomatiq-app-icons.pdf}
    \caption{The application icon variants of Diplomatiq}
    \label{fig:diplomatiq-app-icons}
\end{figure}

\section{Domain name}

\subsection{Overview and purchasing the domain name}

A domain name in the Domain Name System (DNS) represents a network domain, or it translates to an Internet Protocol address~\cite{rfc1035}. Having a domain name is necessary for companies with web-facing services, both for users to easily memorize the address of the service, and for deploying security measures, such as Transport Layer Security (TLS)~\cite{rfc8446}. As Diplomatiq is not primarily a commercial entity, but rather a diplomatic organization, I decided its domain name to be \lstinline{diplomatiq.org}. Since I was already registered at the domain name registrar Namecheap~\cite{namecheap-website}, it was starightforward to purchase the domain name from them, under my existing user account. I set up various DNS records for deployed services, these will be detailed later at describing the services themselves.

\subsection{Security}

Even though I have a secure, long, cryptographically random password for my Namecheap account — as well as for every other user account I have — and I store it in an encrypted password manager, I enabled multi-factor authentication, to reduce the possibility of an unauthorized party accessing Diplomatiq's DNS infrastructure. I also turned on all security alerts to get immediately notified about all events regarding the domain.

I deployed DNS Security Extensions (DNSSEC) under the Diplomatiq domain. DNSSEC essentially prevents spoofing DNS data by providing a set of methods to DNS clients to cryptographically authenticate the integrity and existence (or non-existence) of DNS records~\cite{rfc4033}. Although there is a long-standing debate about the usefulness and operational security of DNSSEC~\cite{4159821, ptacek-dnssec-rant-2, ptacek-dnssec-rant-1}, I decided that until it does not cause any failures or outages in production, it will be enabled.

\section{TLS infrastructure}

\subsection{Overview}

Transport Layer Security (TLS) is a cryptographic protocol with the goal of providing a secure channel between two communicating parties — usually between a client and a server — over the Internet, offering cryptographic assurances for the following:

\begin{itemize}
\item \emph{Authentication} with public-key (asymmetric) cryptography. In TLS, the server is always authenticated, and the client is optionally authenticated.
\item \emph{Confidentiality} with secret-key (symmetric) cryptography. Data sent over the channel is encrypted in transit.
\item \emph{Integrity} with cryptographic message-authentication codes. Data sent over the channel cannot be modified without detection~\cite{rfc8446}.
\end{itemize}

For protecting web application users and web API\footnote{Application Programming Interfaces define interactions between softwares. In this context, a web API means a web-facing service, which serves data for client applications.} consumers on the Internet, a web server should serve its contents over TLS, e.g.\ over the HTTPS protocol, which is essentially HTTP over TLS. More and more web browser APIs require websites and web applications to be served over HTTPS~\cite{secure-context-features}.

Authenticating the server involves a digital certificate, which proves the ownership of the server's \emph{public key}. The public key and its cryptographic key pair, the \emph{private key} are involved in the cryptographic key handshake of TLS, resulting in a symmetric key for encrypting data in transit. Certificates are issued to one or more specific domain names, cryptographically signed by a trusted third party called a Certificate Authority (CA).\footnote{For the sake of compactness, I will not go into the endless details of the X.509 certificate infrastructure and the underlying public-key cryptography in this thesis.} Acquiring such a certificate requires proving the ownership of the domain names in question. \emph{Non-wildcard certificates} only certify domain names they were issued to, whereas \emph{wildcard certificates} certify given domains and all their immediate subdomains.

\subsection{Purchasing a TLS certificate}

Having a TLS certificate signed by a trusted CA is a requirement of serving content over HTTPS, thus it is necessary — but not sufficient on its own — for securing web applications and web APIs. Such certificates can be purchased from a multitude of vendors. For \lstinline{diplomatiq.org}, I purchased a TLS certificate from Sectigo, one of the leading TLS certificate vendors in the world~\cite{sectigo-website}. The certificate is issued to the domain names \lstinline{diplomatiq.org} and \lstinline{www.diplomatiq.org}, and since it is a non-wildcard certificate, I will need to acquire additional certificates for other subdomains.

\subsection{Security}

The private key of a certificate is a highly sensitive secret. An attacker obtaining the private key of a server certificate is able to decrypt all trafic sent to and received by the server, or it can even modify the data in transit, tricking the user into surrendering sensitive information, such as passwords. While being stored securely, the private key needs to be always available to the server, as it is constantly involved in the communication.

The private key of the certificate issued to \lstinline{diplomatiq.org} is encrypted with a long, cryptographically random password, and it is stored in a cryptographic Hardware Security Module\footnote{Hardware Security Modules are separate physical computers designed to keep cryptographic keys safe. They offer tamper resistance making it extremely difficult to extract and steal secret keys~\cite{fips-140-3}.} by Microsoft Azure's Key Vault\footnote{Microsoft Azure and its Key Vault service in particular will be detailed later.} service. This way, the private key is only available in an audited, secure manner, guarded by strict access policies.

As an additional safety measure, I deployed \emph{Certification Authority Authorization (CAA)} DNS records for \lstinline{diplomatiq.org}. CAA records specify authorized CAs, which are allowed to issue certificates for the given domains~\cite{rfc8659}. The records essentially form a CA whitelist, preventing the mis-issue of valid TLS certificates by unauthorized parties. I set up CAA records as granularly as possible: every subdomain has its own authorization. I specified Sectigo for the apex domain \lstinline{diplomatiq.org} and the subdomain \lstinline{www.diplomatiq.org}, DigiCert for the subdomains \lstinline{app.diplomatiq.org} and \lstinline{api.diplomatiq.org}, and Let's Encrypt for the subdomain \lstinline{neo4j.diplomatiq.org}. DNSSEC prevents spoofing additional CAA records into the Diplomatiq domain.

Diplomatiq currently only makes use of server certificates for supporting TLS on its website, web application and web API. Possible future uses of certificates include signing released software with a code signing certificate, or authenticating API clients with client certificates.

\section{Email infrastructure}

\subsection{Overview and usages}

The ability of sending and receiving emails is a basic business requirement for communicating with business clients and service providers. In the following, I will refer these as \emph{individual emails}, as they are mostly initiated by a human individual. Also, application development often demands sending automated \emph{transactional emails} for customers, as well as \emph{promotional emails} delivering marketing campaigns and promotions.

\subsection{Sending and receiving individual emails}

Namecheap offers basic email forwarding capabilities along with its DNS service I subscribed to. Even though certain business email providers offer more robust — and also more expensive — solutions, I decided that my current requirements are covered by forwarding emails received by any address ending with \lstinline{@diplomatiq.org} to my personal email address. I also managed to configure my personal email provider to send emails in the name of several \lstinline{@diplomatiq.org} email addresses through SendGrid, a service for delivering transactional and promotional emails~\cite{sendgrid-website}. For achieving this, I needed to set up separate email entities in my personal email provider to send emails through SendGrid's SMTP-over-TLS API, authenticated by a newly created API key.

Currently several email addresses are configured with the above method, each used for different purposes in different services. Dedicated \emph{per-service email addresses} are useful for services lacking federated authentication and role-based access control features, as they enable to distribute reponsibilities among colleagues by providing access to the email addresses, without irrevocably binding those responsibilities to the colleagues' email addresses. The configured email addresses with their usage are available in the Appendix.

\subsection{Transactional and marketing emails}

For delivering transactional and marketing emails, I subscribed to SendGrid's smallest paid plan, which includes 40,000 sent emails per month. The service allows to create rich-text email templates in its online editor, then send personalized emails to multiple addresses, by substituting template placeholders with per-user customized data. I configured SendGrid to use the \lstinline{team@diplomatiq.org} email address for all outgoing email communication. Configuring the service to access and use the \lstinline{diplomatiq.org} domain was straightforward: it only involved adding generated \lstinline{CNAME} records to the DNS configuration.

\subsection{Securing SendGrid access}

Since SendGrid offers neither federated nor email-based user management, I created a new account with a username. I enabled two-factor authentication for the service, and I defined various alerts for account access and quota usage. I also introduced IP address-based access controls: my user account is accessible only from my static home IP address and my private VPN\footnote{Virtual Private Network}\footnote{I have a personal VPN hosted by a self-configured virtual machine in a data center. The VPN's IP address is also allowed to access SendGrid, so I can log in to my account even if I am not home.}, and the SendGrid API authenticated by my API keys is only accessible from the IP range of the production server infrastructure, which I will detail later. The issued API keys have minimal privileges allowing email sending only.

\subsection{Securing emails sent by Diplomatiq}

Sending emails over the Internet is inherently insecure, as the design of the core email protocols do not incorporate any security features for sender authentication and authorization~\cite{foster2015security}. The infrastructure on its own allows anyone to send emails from any domain, without verifying the authenticity of the sender~\cite{rfc5321}. There are several additional security measures to mitigate this threat, such as the Sender Policy Framework~\cite{rfc7208}, DomainKeys Identified Mail Signatures~\cite{rfc6376}, and the Domain-based Message Authentication, Reporting, and Conformance~\cite{rfc7489} protocols. I have applied all three of them for Diplomatiq.

The Sender Policy Framework (SPF) was designed to detect if the sender address of an email was forged by a party outside the sender's domain. The framework's operation is based on a DNS TXT record\footnote{The SPF record has a specific format, which is defined in RFC 7208~\cite{rfc7208}.} indicating the host or IP address of email servers authorized to send emails originating from the domain. Besides authorized servers, the record also includes instructions for recipient servers and clients on what to do with detected forgeries: such emails can either be forwarded to the recipient's mailbox tagged as spam, or rejected and not delivered. Diplomatiq's SPF policy is configured by Namecheap and SendGrid based on my settings, and it commands to reject emails detected as forgeries.

The DomainKeys Identified Mail (DKIM) Signatures scheme offers similar email sender forgery detection capabilities as the Sender Policy Framework, but also it provides additional assurances supported by public-key cryptography. For utilizing DKIM, the sender server needs to cryptographically sign outgoing emails with a private key, and the domain's administrators need to publish the server's corresponding public key in a DNS TXT record.\footnote{The DKIM record contains additional information besides the key itself. The format of the record is specified by RFC 6376~\cite{rfc6376}.} Recipient email servers and clients can check the authenticity of an email by looking up the sender domain's public DKIM key then verifying the DKIM signature attached to the email\footnote{DKIM signatures are usually verified by email clients rather than end-users, thus the signatures are generally not visible as part of the email.} with the public key. On the one hand, a valid DKIM signature cryptographically guarantees that the email was sent from an authorized party, and on the other hand, it also verifies that the email was not modified in transit. I configured the DKIM records of Diplomatiq to be automatically managed by Namecheap and SendGrid.

The Domain-based Message Authentication, Reporting and Conformance (DMARC) protocol extends SPF and DKIM by allowing domain administrators to explicitly indicate that emails are to be authenticated by SPF or DKIM or both, and to instruct email recipient clients to behave in a specific way when any or all authentications fail, such as rejecting the message, or putting it in quarantine. DMARC also offers a reporting mechanism, which sends reports about authentication failures to a specified email address. Reports can be daily aggregates or real-time \textquote{forensic} reports including detailed data about each failures, or both. Diplomatiq's DMARC policy is set to the strictest: emails not passing all checks should not be delivered to the recipient's inbox. For receiving and analyzing aggregate reports, I use an external tool, called Dmarcian~\cite{dmarcian-website}. DNSSEC prevents the unauthorized modification of Diplomatiq's SPF, DKIM and DMARC policies.

\section{Source code management}

\subsection{Choosing tools}

I have been maintaining altogether 9 projects related to Diplomatiq, all being tracked by a version control system, Git. I chose Git because of its maturity\footnote{Git has been developed since 2005~\cite{git-initial-commit}.} and popularity, and also because of the fact that this is the versioning tool that I am most experienced with. For open-source software maintenance, I chose GitHub, as it is the most popular software collaboration platform~\footnote{GitHub has over 40 million users~\cite{github-user-count}.}, and offers advanced development and project management tools, security settings, and a full-featured, integrated testing infrastructure. Also, it enables to maintain repositories under a larger unit, called organization, offering sophisticated administrative and security features. GitHub is free of charge for open-source projects~\cite{github-pricing}.

\subsection{Registering the Diplomatiq GitHub organization}

I registered an organization on GitHub under the name \emph{Diplomatiq}. I uploaded all necessary data including logos, descriptions, general and billing email addresses, and website addresses. Considering future employees, I made it mandatory for all members of the organization to use two-factor authentication. I verified my ownership of the \lstinline{diplomatiq.org} domain to GitHub with a custom DNS TXT record, thus GitHub displays a \textquote{Verified} label next to Diplomatiq's website address. For making future collaborative development eaiser, I added a set of organization-wide issue and pull request labels, making sure all repositories I create have the same issue types.

\subsection{Standardized, organization-wide documents and configurations}

As I created more projects, I experienced that certain documents, templates and configurations need to be present in all of projects, mainly because of the recommended open-source community standards maintained by GitHub~\cite{opensource-guide}. For being able to instantly create a new project with Diplomatiq's standardized project structure containing all such necessary files, I created another project called \emph{project-config}, which will be described in \Cref{chapter:libraries}.

\subsubsection{License}

The most basic project requirement is a license. Within Diplomatiq, project licenses are stored in a file called \lstinline{LICENSE} within the project's root directory. This is in line with GitHub's recommendations, thus the platform can automatically parse and display license information. Currently all Diplomatiq projects are licensed under the MIT License~\cite{mit-license}.

\subsubsection{Readme}

All projects are initialized with a \lstinline{README.md} file containing the project's name and description. I put all project documentation into the readme file for most projects, thus these files are not left empty. I usually also include badges into readme files about build status, versioning and license information, as well as code quality and code coverage data.

\subsubsection{Code of conduct}

Code of conduct documents formulate a set of ethical norms and responsibilities on practices of collaboration. Diplomatiq uses the version 1.4 of the Contributor Convenant's Code of Conduct across all its projects, in a file named \lstinline{CODE_OF_CONDUCT.md}. Even though currently there is no open-source collaboration in Diplomatiq's repositories, once we get there, I want to take the Code of Conduct very seriously, in order to create a welcoming and inclusive development environment. I have already configured the \emph{conduct@diplomatiq.org} email address for reporting unacceptable behaviour.

\subsubsection{Contributing information}

The document \lstinline{CONTRIBUTING.md} — named in line with GitHub's open source recommendations — summarizes how to contribute to Diplomatiq projects. It describes means of communication with project maintainers, and methods of requesting features and reporting bugs. It formulates a set of submission guidelines for issues and pull requests. It also presents a style guide for the code itself, and for commit messages. As I adopted the Angular-style Conventional Commits specification~\cite{conventionalcommits}, the document introduces the concept conventional commit messages in an example-based manner.

\subsubsection{Pull request template}

In order to discourage future contributors from submitting incomplete work, I prepared a checklist as part of GitHub's pull request template. As the checklist is saved into the file called \lstinline{.github/PULL_REQUEST_TEMPLATE.md}, GitHub displays it as the initial contents of the to-be-submitted pull request's details field. The checklist requires:

\begin{itemize}
\item the pull request to consist of one commit;
\item the commit message to follow the commit message guidelines;
\item the tests to be updated (if applicable);
\item the documentation to be updated (if applicable).
\end{itemize}

Beyond the checklist, the template instructs the developer to indicate the pull request's type, describe the application behavior the pull request modifies, describe the new behaviour, and indicate if the pull request introduces a breaking change.

\subsubsection{Issue templates}

In open-source projects, it is common that developers submit issues without describing clear and concise details. To discourage this routine, I created two types of issue templates. When developers want to create a new issue in a repository, they are offered these issue templates to choose from. The \emph{bug report} template encourage developers to detail the perceived failure as much as possible, and to disclose the execution context, expected behaviour and steps of reproduction. The \emph{feature request} template supports the issuer in describing their exact requirements along with considered solutions, if there are any.

\subsubsection{.editorconfig file}

EditorConfig helps maintain consistent coding styles for multiple developers working on the same project across various editors and IDEs. The EditorConfig project consists of a file format for defining coding styles and a collection of text editor plugins that enable editors to read the file format and adhere to defined styles~\cite{editorconfig}.

\subsubsection{Security policy}

Since exploiting security vulnerabilities can lead to catastrophic consequences like data breaches or loss of data, security issues are to be handled carefully and discreetly. A security policy establishes rules and methods for reporting a security issue in a development project to the maintainers without causing any harm. Diplomatiq's security policy requires contributors to report found security issues to the \emph{security@diplomatiq.org} email address, possibly encrypted with Diplomatiq's public PGP key.\footnote{Diplomatiq's public PGP key is available on the https://www.diplomatiq.org/pgp-key.txt URL.}

\subsection{Development and release model}

In most of Diplomatiq's repositories, I use the a modified version of the GitFlow workflow~\cite{gitflow}. In essence, the workflow operates on four types of branches in a repository:

\begin{enumerate}
\item The \emph{master} branch is stable, and should be kept stable at all times. Code gets merged into the master branch only from a \emph{release} branch, immediately before a new version of the software is released into production.\footnote{As described later, most Diplomatiq projects are configured in a way that merging a tagged commit into the master branch triggers the continuous delivery flow and deploys the version into production.} Contributors can not push code directly onto this branch, but administrators can.
\item The \emph{develop} branch is part of the repository's regular development flow. When creating feature branches, contributors branch from develop, and the target of contributional pull requests is the develop branch. Contributors can not push code directly onto this branch, but administrators can.
\item The \emph{feature/*} branches are also parts of the regular development flow. Contributors develop their contributions on feature branches, regardless of the contribution's type (feature, bugfix, refactor, etc.). Contributional pull requests are created from feature branches targeting develop.
\item The \emph{release/*} branches are created when a release candidate is being prepared, by branching from develop. Release-related code changes like version bumps, changelog generation and release hotfixes are committed onto the release candidate's release branch, and eventually the finalized release candidate is tagged. After conducting all necessary testing on the release candidate, the release branch is merged into master, then it is immediately released into production by continuous delivery. After the deployment finished, the release branch is merged into develop. Contributors can not push code directly onto release branches, but administrators can.
\end{enumerate}

\Cref{fig:git-workflow} displays the above workflow, which can be well supported by GitHub's continuous integration and continuous delivery capabilities and branch protection rules. I created such rules for the \emph{master}, \emph{develop}, and \emph{release/*} branches. All branches require certain status checks to pass before merging code into the mentioned branches, and require the merged branches to be up to date when merging. As a general rule, I also require linear history in Diplomatiq's repositories, because I experienced that this way tasks such as automated changelog generation or finding regressions with \lstinline{git bisect} becomes easier.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/git-workflow.pdf}
    \caption{Diplomatiq's modified version of the GitFlow workflow}
    \label{fig:git-workflow}
\end{figure}

I implemented a versioning strategy following the scheme of semantic versioning. This scheme defines the software version as a numeric triplet separated by dots in the form \lstinline{MAJOR.MINOR.PATCH}. The \lstinline{MAJOR} version is to be incremented in case the version contains incompatible API changes, the \lstinline{MINOR} version is to be incremented if the version's added functionality is backwards compatible, and the \lstinline{PATCH} version is to be incremented if the version solely consists of backwards compatible bug fixes~\cite{semver}. Adhering to the above helps managing software versions and dependencies with a lesser chance to introduce unintended breaking changes.

As Diplomatiq's development and release model features both conventional commits and semantic versioning, it was straightforward to implement automated changelog generation as well. Using Conventional Changelog Tools~\cite{conventional-changelog}, I automatically generate each released version's changelog from the commit messages contained by the version.

\subsection{Automated dependency management}

As development and release processes become more and more automated, the time frame required for releasing new software versions reduces. With continuous delivery, pushing new code into the remote repository often means that the code is released to production without further developer interaction, after all necessary automated tests pass. This results in the need of more frequent dependency updates as well. Maintainers can either check and update every dependency manually, or they can apply automated solutions.

I configured Dependabot for the automatic maintenance of dependencies across Diplomatiq projects. Dependabot is a GitHub-owned, integrated tool for watching package managers like NPM and Maven, continuously checking for new versions published. I set up Dependabot in a way that if it detects one of the dependencies in Diplomatiq projects having a new version released, it opens a pull request in the related project with the dependency update. This automated workflow frees me from a lot of manual work, and also ensures that the dependencies of Diplomatiq projects are always up to date. Pull requests containing security-related updates are separately tagged, allowing me to handle them with higher priority. As I do not want to lose the ability of reviewing dependency updates, merging pull requests opened by Dependabot is not automated. But since continuous integration workflows are applied to all pull requests, all checks and tests defined in the repository are run for Dependabot's updates as well as for any other contribution.

\subsection{Continuous integration and continuous delivery}

Martin Fowler defines continuous integration (CI) as a software development practice where team members integrate their code changes into the repository mainline frequently, at least daily, and each integrations are verified by a set of automated build and test methods in order to detect integration errors as soon as possible. According to Fowler, this leads to less integration problems, and faster development and release cycles~\cite{fowler-ci}.

In case of Diplomatiq projects, the repository mainline is the develop branch, and the integrations are essentially pull requests opened by repository contributors. The set of automated build and test methods applied to pull requests vary across projects according to the chosen technology, language, and framework, but Diplomatiq libraries and production software usually contain automated tests for verifying conformance, functional correctness, and adherence to various code quality metrics.

I configured all projects to use GitHub Actions~\cite{githubactions} for continuous integration (and continuous delivery). Previously I used the mature and well-supported Travis CI~\cite{travisci}, but encountering constant stability problems with Travis made me to switch to the then freshly announced GitHub Actions. Since the switch, I experienced GitHub Actions to be a well-founded, stable solution. As it is an integral part of the GitHub platform, it furthers the possibilities of integrations by allowing to subscribe to more repository events than external platforms~\cite{actions-events}. It also enables to utilize GitHub's REST and GraphQL APIs for extending the base functionality of GitHub Actions with built-in authentication — a feature I make use of in almost all projects.

GitHub Actions essentially allows to create custom, automated \emph{workflows} for various phases of the software development lifecycle. Such workflows are declaratively defined in a YAML\footnote{Yet Another Markup Language is a markup language for human-readable configuration and serialization.} files, stored in the repository alongside the project's sourcecode. Workflows are triggered by GitHub's \emph{events}, and consist of \emph{jobs} able to run sequentially on different {runners} having different operating systems. Jobs encompass \emph{steps}, the smallest work unit within a workflow. Steps are either predefined \emph{actions} stored in public repositories and created by GitHub members, or shell scripts.

Although different projects require different CI workflows, I have established the following baseline for checking contributions across Diplomatiq projects:

\begin{itemize}
\item If the workflow was triggered by a pull request, check if the pull request consists of one commit. This involves substeps of querying the number of commits for the pull request through GitHub's GraphQL API, extracting the number of commits from the JSON response, and checking if the number is equal to 1.
\item Check if the commit message conforms to the requirements of Angular-style conventional commits. The commit message is linted with the \emph{commitlint} tool, part of the previously mentioned Conventional Changelog Tools.
\item After setting up the project's programming language environment and installing all necessary dependencies, perform code linting in order to verify that the submitted code conforms to the required code styling of Diplomatiq. The code is linted with language-dependant tools.
\item Verify that the project can be built by creating a distribution deployment artefact.
\item Run all unit, integration, and end-to-end tests of the project, if there are any.
\item Scan the code with SonarQube, and upload the results to SonarCloud.
\end{itemize}

If any of the above steps fails, the contrubition should be considered as faulty, and it should not be accepted into the mainline. Since all Diplomatiq projects are open-source, and GitHub Actions is offered free of charge for open-source projects, I usually run workflows incorporating the above steps against multiple platforms, operating systems, and multiple versions of programming language environments concurrently, without additional costs.

I also utilize GitHub Actions for continuous delivery. Most projects are configured in a way that pushing a tagged commit to the master branch triggers the release process. The process consists of producing and uploading a distribution build artefact to GitHub's permanent artefact storage, running all tests against all environments on the artefact, and if every step is successful, deploying the artefact into the production infrastructure.

\subsection{Code analysis with SonarCloud}

SonarSource's SonarQube along with its cloud platform, SonarCloud are tools for inspecting code quality based on static code analysis. Both examine the code using the same configurable — and in case of SonarQube, extensible — analysis ruleset in order to detect bugs and security vulnerabilities. SonarQube is to be used in on-premise, SonarCloud is to be used in cloud-based development environments, and both can be included in continuous integration environments.

As it is free to use for open-source development, I configured SonarCloud for most Diplomatiq projects. After creating the Diplomatiq organization on the platform, I imported the Diplomatiq's GitHub repositories into SonarCloud through an automated flow. I created a token for each project, which authenticates the project against SonarCloud's API when performing analyses and uploading results.

SonarCloud introduces the concepts of \emph{quality profiles} and \emph{quality gates}. A quality profile is a set of language-dependant quality rules, which get applied against the codebase during an analysis. Profiles are categorized into four parts:

\begin{enumerate}
\item \emph{Bugs} are development errors which occasionally result in faulty software.
\item \emph{Vulnerabilities} are pieces of code that causes the software to become vulnerable against attacks.
\item \emph{Code smells} are bad coding practices which make the code harder to maintain.
\item \emph{Security hotspots} are subject to become vulnerabilities if applied incorrectly.
\end{enumerate}

An example of a quality rule categorized as a bug in the Java quality profile is the reversion of the \textquote{not equals} operator (incorrectly using \lstinline{=!} instead of the correct \lstinline{!=}): the code compiles, but the operator does not produce the intended results. Quality profiles can be customized by including or excluding specific rules. When using SonarQube, new quality rules can also be implemented for several languages, but SonarCloud can not be extended with custom rules~\cite{sonar-custom-rules}. For Diplomatiq projects, I use the quality profiles recommended and maintained by SonarSource.

A quality gate is a set of boolean conditions based on measurement values. Such conditions are not dependent on any language or platforms, since it formulates general measurements, such as code coverage should be kept above a specific percentage, the number of bugs should be equal to zero, or security rating should be the highest. A quality gate \textquote{lets through} code which complies with all its requirements. For Diplomatiq projects, I use a custom quality gate with the highest and strictest possible settings on the platform.

As I configured SonarCloud to be part of the continuous integration workflows of Diplomatiq projects, only such pull requests can be merged into a Diplomatiq repository, which passes all requirements I set in SonarCloud. Since the CI rules of GitHub projects apply to me as well as any other future contributor, this enforces even me to write code with consistently high quality and 100\% test coverage.\footnote{I always strive to produce high quality code and avoid bugs. However, SonarCloud helped me to notice two minor bugs so far, which were not reported by tests, even with 100\% test coverage.}

\section{Node Package Manager (NPM)}

\subsection{Overview}

The Node Package Manager (NPM) — part of an ecosystem built upon the JavaScript runtime environment Node.js — is a package manager for software written in JavaScript. NPM's online package registry containing public and private packages enables developers to download and re-use shared packages by incorporating them into their own software in a versioned manner, with the help of NPM's command line client. It has become the de-facto standard solution for sharing JavaScript software in the open-source community~\cite{herron2016node}.

\subsection{Creating and securing an organization}

I implemented several JavaScript libraries to be published as open-source software, but in Diplomatiq's name instead of my personal account. For this, I created an organization on NPM, similarly to GitHub. Organizations can publish packages in a scoped way: the name of the organization prefixed with a \textquote{@} character — the scope name — becomes the prefix of the package's name. The scope and the package name are separated with a \textquote{/} character. For example, the \lstinline{project-config} package published under the Diplomatiq organization gets finally named \lstinline{@diplomatiq/project-config}\footnote{NPM scopes can be lowercase only, so according to Diplomatiq's naming rules described in \Cref{section:naming}, the organization's scope has been \lstinline{@diplomatiq}.}.

NPM offers no security policies for organizations. As a security measure, all I was able to do was to enable two-factor authentication in my account for publishing packages as well, as it was of course already enabled for authentication. This means that if I invite a user into Diplomatiq's NPM organization, they will be able to publish packages even if they do not have two-factor authentication enabled. This way, an attacker stealing the password of one of the organization's future accounts can easily compromise Diplomatiq packages. I contacted NPM's customer support about mitigating this future threat, and I was told that the functionality of organizational policies are on their development roadmap, expected to be released to production in the fourth quarter of 2020.

\section{Communication and social media presence}

\subsection{Gitter}

I created a space for Diplomatiq on Gitter, a chat service for GitHub repositories and developers. The space was named Diplomatiq, according to the primary naming rule in \Cref{section:naming}. Gitter allows to create several chat rooms within Diplomatiq's space, corresponding to the repositories within Diplomatiq's GitHub organization. As advised in Diplomatiq's contributing guide, the Gitter space should be the primary place for discussing questions and problems regarding Diplomatiq projects. At the time of writing this thesis, I am the sole member of Diplomatiq's Gitter space, and there has been no communication in the chat rooms.

\subsection{Slack}

For future internal communication, I registered a workspace for Diplomatiq on Slack, one of today's most popular business communication platforms. The workspace was named Diplomatiq, but due to the \emph{diplomatiq.slack.com} domain being already taken, its domain name has been \emph{diplomatiqorg.slack.com}, according to the 4\textsuperscript{th} naming rule in \Cref{section:naming}. Even though the workspace is unused as I am its sole member, reserving \emph{diplomatiqorg.slack.com} ensures that the domain name will not be taken by another company. As a security measure, I made it mandatory for all future members to use two-factor authentication.

\subsection{Facebook}

I created and published a Facebook page for Diplomatiq, reserving the Facebook page path \emph{https://www.facebook.com/DiplomatiqOrg}, according to the 3\textsuperscript{rd} naming rule of \Cref{section:naming}. The page has no posts and contains no data, but I uploaded one of Diplomatiq's application icon as its profile picture, and the logo as its cover picture. In the future, I want to heavily build on Diplomatiq's Facebook presence for reaching the younger generation of Model United Nations.

\section{Procuring the Neo4j database software}

\subsection{Licensing overview}

I chose the Neo4j graph database as the main database serving the Diplomatiq social network application. The reasons of my decision are detailed in \Cref{section:database}, this section focuses on licensing and the procurement of the database software.

As already mentioned in \Cref{subsection:preliminaryneo4j}, Neo4j offers two software editions with different feature sets and licensing~\cite{neo4j-licensing}. The Community Edition is open source, available under the terms of the GPL v3 license. The Enterprise Edition offers four licensing options:

\begin{itemize}
\item The commercial license permits using the software in closed source applications as well, and is offered under a paid subscription agreement.
\item The developer license permits free local development usage after registration.
\item The evaluation license is a time-restricted variant of the commercial license, offering free usage for a fixed-term trial period.
\item The startup license permits using the software for free for startups having at most 50 employees and \$3 million of annual revenue, but limits the number of database instances to 3 production, 3 staging, and 6 testing/development machines, each having 256 gigabytes of RAM and 24 processor cores at most~\cite{neo4j-startup-program}.
\end{itemize}

Besides the software itself, Neo Technology offers another approach for using their graph database: Neo4j Aura is a managed graph DBaaS\footnote{database as a service}, offering on-demand scaling, a capacity-based pricing construction, and clustering features for high availability.

\subsection{Acquiring the startup license}

After evaluating licensing options and the Neo4j Aura service, I decided that since Aura is too expensive for my current budget, I enrolled in the startup program. This involved filling a registration form with personal data and information about the (prospective) company. Two days after submitting the form, I received the email with a license key for Neo4j Enterprise Edition. This enabled me to download and deploy the software onto Diplomatiq's server infrastructure.

\section{Building and securing the server infrastructure}

\subsection{Platform overview}

In this section, I use the term server infrastructure to denote the various servers, software, networking solutions, and configurations that enables and supports the public operation of the Diplomatiq social network software system on the Internet (excluding DNS and emailing). Having relevant work experience in operating a production cloud server infrastructure, I had a concrete concept on Diplomatiq's demands on the short and on the run as well. The key aspects of choosing the platform were the following:

\begin{itemize}
\item It should provide strong cloud-based services with hybrid cloud\footnote{A hybrid cloud consists of cloud-based and on-premise infrastructure elements.} capabilities for later expansion into real-world diplomacy.\footnote{According to my knowledge, governmental and diplomatic software often require on-premise solutions.}
\item It should provide strong PaaS\footnote{Platform-as-a-Service} capabilities, and also IaaS\footnote{Infrastructure-as-a-Service} solutions.
\item It should provide options on data residency to comply with diplomatic requirements.
\item It should offer a flexible pricing model, so I can start with a smaller, cheaper infrastructure, and scale it later as Diplomatiq's production workload grows.
\item It should provide mature, integrated security solutions.
\item It should provide affordable developer support.
\end{itemize}

Considering the above, I evaluated service offerings of Microsoft Azure, Google Cloud Platform, and Amazon Web Services. Google seems to lag behind on hybrid solutions, and also seems to be the worst from the financial aspect for Diplomatiq's use cases. Even though Amazon is the oldest cloud service, it seems to lack integrated data residency solutions. Microsoft seems to offer everything Diplomatiq will need in the long run, and it proved to be the best in terms of pricing, therefore I chose Microsoft Azure as Diplomatiq's cloud server infrastructure platform.

\subsection{Creating a directory for Diplomatiq}

\subsubsection{Overview}

In enterprise infrastructures, the biggest organizational unit is usually called a \emph{directory}. A directory encompasses the entire organization with its users, computing resources, and often includes information about premises and office resources as well. In Microsoft Azure, a directory — also called \emph{directory tenant} — is managed via the service called Azure Active Directory (AD).

\subsubsection{Creating a personal directory with my personal Azure account}

I could not sign up to Azure by simultaneously creating a directory dedicated to Diplomatiq (I was later told by support that this is only possible by signing up through Microsoft salespersons, which I intentionally avoided). Instead, I needed to create a personal Microsoft account with my personal email address, which initialized my personal directory and set its domain name to \emph{luczsomagmail2559.onmicrosoft.com}, based on my email address.

\subsubsection{Creating a directory dedicated to Diplomatiq}

For Diplomatiq's own, dedicated directory, I needed to create a separate directory tenant from my personal directory. For this dedicated directory tenant, I set Diplomatiq as the organization name, and as its initial domain name, I set \emph{diplomatiq.onmicrosoft.com}. As I created the new directory tenant from my personal Microsoft account, the \emph{global administrator} — the highest possible role able to manage all services and everything else in a directory — of the new tenant became my personal account.

\subsubsection{Setting diplomatiq.org as the primary domain in Diplomatiq's directory}

In order to be able to invite users with \emph{@diplomatiq.org} email addresses, the ownership of the \emph{diplomatiq.org} domain needed to be verified to Microsoft. I registered \emph{diplomatiq.org} as a custom domain name for the directory, and proved the administrative ownership of the domain by adding a Microsoft-provided identifier as a DNS TXT record. After the verification, I set the domain to be the primary one, leaving \emph{diplomatiq.onmicrosoft.com} as a secondary directory domain.

\subsubsection{Creating a company Azure account and making it administrator}

After Diplomatiq's domain name was verified in Azure, I was able to create a new user in Diplomatiq's dedicated directory with the \emph{soma.lucz@diplomatiq.org} email address. I assigned the global administrator role to this new account, making it administratively equivalent to my personal account.

\subsubsection{Deleting the personal account}

After I had completely set up Diplomatiq's dedicated directory and had configured its \emph{soma.lucz@diplomatiq.org} user to be a global administrator, there was no need for keeping my personal Azure account, so I deleted it from the directory. Later I deleted the account itself, which deleted its personal directory as well.

\subsubsection{Further directory settings and security considerations}

I performed further configuration to restrict future users' access within the directory in order to keep responsibilities limited and separated. I disabled user application registration, to disallow arbitrary users to issue application credentials accessing resources in the directory. I restricted the the access to the AD's administration portal, so users are not able to modify AD settings. I made it mandatory for all future users in the directory to use two-factor authentication.

\subsection{Naming conventions}

Microsoft Azure has a detailed guide as part of its Cloud Adoption Framework on how to name management groups, subscriptions, resource groups, and resources themselves~\cite{azure-naming}. Since Azure does not allow to rename a resource after its creation, I wanted to establish a solid system of naming conventions. I downloaded the naming ang tagging convention tracking template provided by Azure, and decided to stick to it. As the table does not contain conventions for all resource types, I sometimes needed to extend it with new rules. Henceforth every resource name I introduce was named according to Diplomatiq's naming conventions.

\subsection{Creating a subscription and a support subscription}

Within a directory — the largest organizational unit — there can be several management groups nested into each other, as the second level of resource organization. Management groups contain subscriptions (or other management groups), within subscriptions there are resource groups, and resource groups consists of resources. Since I decided Diplomatiq only needs one subscription to encompass all resources, its directory would make no use of management groups.

I set up the \emph{diplomatiq-prod-001} subscription paid by my credit card as a pay-as-you-go subscription. This allows me to pay for the resources I used in a time-based manner: the more resources I use (or the higher their pricing tiers are\footnote{Higher pricing tiers offer more features or higher performance.}), the more I pay. I additionally subscribed to the \emph{Developer support plan} later to resolve a networking issue with the Key Vault service.

\subsection{Structuring resources}

As part of its global infrastructure, Microsoft Azure provides services in more than 60 \emph{regions} worldwide. A region is a set of datacenters within a \emph{geography} (usually a country), which acts as a data residency boundary~\cite{azure-global-infrastructure}. Deploying infrastructure elements across several geographies allows building high-availability, low-latency systems — the closer the infrastructure is to the client, the quicker the content can be delivered over the Internet.

Regions are bound to resource groups. Since the current needs of Diplomatiq do not justify deploying multi-region services, I created only one resource group in the Microsoft's North Europe (Ireland) region, named \emph{rg-diplomatiq-prod-001}. All resources presented later were deployed into this resource group. I chose the North Europe region because it is one of the oldest and best-supported Azure regions, and it is also among the regions having the highest service coverage in the Azure infrastructure. Later this decision proved problematic: North Europe is also one of the most popular regions, and due to the heavy interest in cloud solutions raised by the COVID-19 pandemic, Microsoft introduced limitations on new infrastructure deployments. Because of this, initially I was not able to deploy a virtual machine for the Neo4j graph database.

\subsection{Securely storing secrets and credentials}

Maintaining an IT infrastructure involves dealing with several kinds of credentials, keys, and certificates. Most of these are highly sensitive secrets; letting an attacker steal them could cause critical service disruption or even permanent damage. Microsoft Azure offers its Key Vault service for storing secrets in a secure and audited manner, guarded by configurable access policies.

The Key Vault service is offered in two tiers:

\begin{itemize}
\item The \emph{standard} tier provides all security and auditing functionalities of the Key Vault service, except for storing keys in Hardware Security Modules (HSMs).\footnote{As mentioned earlier, Hardware Security Modules are separate physical computers designed to keep cryptographic keys safe. They offer tamper resistance making it extremely difficult to extract and steal secret keys~\cite{fips-140-3}.}
\item The \emph{premium} tier provides the optional usage of Hardware Security Modules for storing keys, in addition to all features offered by the standard tier.
\end{itemize}

Diplomatiq's infrastructure incorporates several kinds of secrets, which I decided to store in the Key Vault. Since the two tiers' prices differ only minimally, I chose the premium tier, offering the optional usage of HSMs. I created two Key Vault instances. The \emph{kv-sslcerts-prod-001} instance stores TLS certificates necessary for secure communication with clients and across services. The \emph{kv-prodcreds-prod-001} instance stores everything else: database access credentials, database encryption keys, and API keys of integrated services.

Key Vault access policies can be fine-tuned in various ways. Each entity within the directory can have a separate set of access to the actual cryptographic operations performed by Key Vault instances. I configured the access policies of the instances to be as strict as possible: service identities\footnote{Azure provides a built-in way to identify and authenticate a service in a directory, called \emph{service identity}.} required to access entries as part of their normal operation are able to \emph{get} an entry by referencing it by its identifier, but they can not \emph{list} entries, and they have no write access at all. As an additional security measure on top of authenticated and audited access, the Key Vault instances are not reachable from the Internet, only from a specified internal subnet the backend services are deployed to.

\subsection{Setting up the infrastructure for the website}

\subsubsection{Overview}

Diplomatiq's website can be reached on the address \emph{www.diplomatiq.org}. I created the website for conducting future marketing operations, and to introducing the company. The website is separated from the Diplomatiq application itself, available on \emph{app.diplomatiq.org}.

Serving a website over the Internet requires a public-facing webserver or — if the website consists of only static elements without application logic implemented on the server — a serverless, static web storage. Both approaches have benefits: running a webserver allows a deeper level of configuration, but a static web storage can more easily be replicated around to world to enable serving websites with lower latency. Due to lack of important security capabilities of static web storage technologies in Azure — e.g. adding headers for implementing Content Security Policy\footnote{Content Security Policy is an important security measure for websites and web application, mitigating certain types of attacks, such as Cross-Site Scripting (XSS)~\cite{csp-mdn}.} —, I decided that the website infrastructure should be implemented on a webserver-based approach.

Microsoft Azure provides a mature Platform-as-a-Service (PaaS) technology called \emph{App Service}. In contrast to Infrastructure-as-a-Service (IaaS), one does not need to maintain the infrastructure itself when using a PaaS: the underlying computers are abstracted, and the operating system and server software are continuously updated. App Services are essentially managed PaaS infrastructures with built-in scaling, load balancing, and security features. As I have relevant work experience on the maintenance of production App Services, I decided to use an App Service for serving Diplomatiq's website.

\subsubsection{Creating a suitable App Service Plan}

Azure requires having an App Service Plan for deploying App Services. While App Services are in essence instances of web applications (consisting of multiple actual servers and infrastructure, which is abstracted and invisible to the user), App Service Plans can be described as server farms, which can comprise several App Services. App Service Plans are offered in several pricing tiers, each providing different hardware — either in a shared or isolated virtualized environment — with differing performance, and varying features.

I created an App Service Plan named \emph{plan-diplomatiq-prod-001}, and determined its pricing tier based on the following criteria:

\begin{itemize}
\item It should support HTTPS. Since all pricing tiers support HTTPS, this requirement does not exclude any tiers.
\item It should support adding custom domain names, so the application can be served from \emph{diplomatiq.org} instead of a domain name ending with \emph{azurewebsites.net}. This excludes smaller tiers targeted for development or testing usage.
\item It should support multiple deployment slots\footnote{A deployment slot is a separated instance of the same web application available under the same domain name. The \emph{production} slot gets served by default, and the other slots can be visited by sending a cookie along the request, which instructs the load balancer in front of the slots to route the request to the targeted one.} for conducting testing in production\footnote{Testing in production involves running the System Under Test (SUT) under the same conditions as the SUT runs normally in the production environment.}, and swap-based, zero-downtime deployment.\footnote{With swap-based deployments, a new version is deployed into another deployment slot, then the slot gets swapped with the production slot to receive all incoming traffic, without downtime.}
\end{itemize}

The cheapest tier supporting the required features listed above was the smallest tier targeted for production use, called S1, therefore I chose this for \emph{plan-diplomatiq-prod-001}.

\subsubsection{Creating and configuring the App Service}

I created an App Service for the website named \emph{app-diplomatiqwebsite-prod-001}, and deployed it into the \emph{plan-diplomatiq-prod-001} App Service Plan. I configured it to be reachable under the \emph{diplomatiq.org} and \emph{www.diplomatiq.org} domain names. This involved adding a CNAME DNS record for the \emph{www} subdomain with a value specified by Azure, and an A DNS record for the apex domain, pointing to the public IP address of the App Service.

I chose the \emph{canonical domain name} of Diplomatiq's website to be the \emph{www.diplomatiq.org}. Due to security, load-balancing and administrational issues, choosing a canonical name and sticking to it is an important aspect of maintaining a web application~\cite{mdn-canonical-domain, bjorn-canonical-domain}. I configured the platform in a way that requests targeting the \emph{diplomatiq.org} apex domain also reach the web server, but the server should issue a \emph{permanent redirect} to \emph{www.diplomatiq.org} with the HTTP status code 301 in this case.

After setting up the domain names, I configured the App Service's TLS settings. I configured the webserver to mandate clients to use HTTPS with the 1.2 version of the underlying TLS protocol\footnote{Mandating the client to use HTTPS means that requests using the HTTP protocol are redirected to \emph{https://www.diplomatiq.org} with a \emph{permanent redirect} having HTTP status code 301. It also means configuring HSTS properly, described later.}. In order to use HTTPS, I needed to provide the previously introduced Sectigo TLS certificate issued to the \emph{diplomatiq.org} and \emph{www.diplomatiq.org} domains. I uploaded the certificate along with its password-protected private key into the \emph{kv-sslcerts-prod-001} Key Vault, and used the automated attachment process to bind them to the App Service.

\subsubsection{Zero-downtime deployment}

At this point, the infrastructure for securely serving a website was ready. In order to be able to deploy new application versions without downtime, I cloned the default \emph{production} deployment slot with all its settings, and created a \emph{staging} slot. With the swap-based deployment process provided by App Services, new website versions can be deployed onto the staging slot, and — as the users' incoming requests are routed to the production slot by default — the staging slot can be reconfigured then restarted according to the new version's demands, without causing service disruption. After the necessary reconfiguration is finished, and the staging slot's server instances are warmed up for receiving production workload, the staging slot can be swapped into production without downtime. The automated swap operation essentially consists of modifying the configuration of the slots' load balancers to route incoming production traffic to the new slot. This reconfiguration is performed without restarting any infrastructure elements, thus without downtime.

The deployment itself is performed as part of the continuous delivery workflow introduced earlier. Azure provides a set of configurable actions (as in GitHub Actions) to be incorporated into automated deployment procedures. I integrated such an action into the website's deployment workflow, which automatically deploys the built artefact into Azure. The deployment is authenticated with the App Service's publish profile, which is embedded into the repository as a GitHub encrypted secret~\cite{github-encrypted-secrets}.

\subsubsection{HTTP Strict Transport Security (HSTS) preloading}

Compatibility reasons justify that webservers also serve entry points for web applications over HTTP as well as HTTPS, so a user visiting e.g. \emph{http://www.diplomatiq.org} (note the usage of HTTP instead of HTTPS) is eventually redirected to the website instead of receiving an error. The primary and only goal of HTTP entry points should be to redirect the user to the HTTPS entry point of the application. The \emph{HTTP Strict Transport Security (HSTS)} browser mechanism furthers this approach by allowing servers to specify a \lstinline{Strict-Transport-Security} header in their response, instructing the browser to permanently remember that the site should only be visited over HTTPS in the future~\cite{garron2013state}. If an HSTS entry exists in the browser for a given site, and the user tries to visit the site over HTTP, the browser automatically upgrades the connection to HTTPS, before even making an actual request to the site.

HSTS is based on the \emph{trust on first use} principle, meaning its protection only applies after a user visited the site the at least once. This means that if an attacker hijacks the very first HTTP request a user made to a site, they can impersonate the server and serve malicious content to the user~\cite{rfc6977}. Major web browsers, such as Google Chrome, Mozilla Firefox, Apple's Safari, Microsoft Internet Explorer 11 and Microsoft Edge solve this problem by packaging a HSTS preload list into the web browser application: the list specifies sites supporting HSTS~\cite{hstspreload}.

I have enabled HSTS for all Internet-facing Diplomatiq services, and also added the \emph{diplomatiq.org} domain name (and implicitly all its subdomains) to Google Chrome's HSTS preload list.\footnote{The automated submission process can be initiated on https://hstspreload.org.} Most major web browsers build their own lists upon this list~\cite{hstspreload}.

\subsection{Setting up the infrastructure for the front end application}

\subsubsection{Overview}

In terms of infrastructure requirements, Diplomatiq's front end application is very similar to the website. Since the application is a single-page client application, it does not need any back end logic, apart from routing and security capabilities. I created the same infrastructure with the same configuration for the front end application as for the website. It is hosted by an App Service in the same App Service Plan as the website, and its deployment model is also the same as the website's. There are two differences: it is served under a different subdomain, and it has one more deployment slot besides production and staging.

\subsubsection{Subdomain settings, acquiring a new TLS certificate}

I set up the resource to be served under the \emph{app.diplomatiq.org} domain name. Similarly to the website, this involved adding a CNAME DNS record for \emph{app.diplomatiq.org} with a value provided by Azure. As the Sectigo TLS certificate is issued to the \emph{diplomatiq.org} and the \emph{www.diplomatiq.org} domain names, it can not be applied to this resource. For being able to serve the application over HTTPS, I needed to acquire a new TLS certificate for its subdomain. Via a fully automated process, I created a managed TLS certificate offered by the App Service\footnote{Azure managed TLS certificates are issued by DigiCert~\cite{digicert-website}, thus I needed to add another CAA record to Diplomatiq's DNS settings to allow DigiCert issuing a certificate to \emph{app.diplomatiq.org}.}, saved it into the \emph{kv-sslcerts-prod-001} Key Vault, then attached it to the App Service. As their name suggests, managed certificates are automatically managed and renewed by Azure~\cite{managed-certificates}, freeing me from further maintenance tasks related to TLS.

\subsubsection{Deployment slots}

As detailed earlier, the development model of Diplomatiq features a branch named \emph{develop} as its semi-stable mainline branch for day-to-day development. Besides the \emph{production} and \emph{staging} slots, I created a deployment slot named \emph{develop} as well. After a pull request is merged into the develop branch in the front end project's repository, the develop branch is automatically released to the develop deployment slot. This is similar to the concept of nightly builds, only in this case updates can happen more frequently, as more pull requests are merged into the develop branch.

\subsection{Setting up the infrastructure for the back end application}

\subsubsection{Overview}

The back end application of the Diplomatiq social network is exposed to clients by an API available over HTTPS. The API and the application itself will be detailed in \Cref{chapter:diplomatiq}, but since the application needs a Java runtime environment, it evidently needs a webserver as well, which forwards the incoming requests to the application. Similarly to the website and the front end, I decided to use an App Service for hosting the back end application.

\subsubsection{Subdomain settings and another new TLS certificate}

I set up the resource to be served under \emph{api.diplomatiq.org} domain. As the Sectigo TLS certificate can not be applied to this resource either, I created a managed TLS certificate and saved it into the \emph{kv-sslcerts-prod-001} Key Vault via the same process I already mentioned.

\subsubsection{Deployment slots}

The application features the same three-slots deployment model as the front end application, with the same naming and same semantics.

\subsubsection{Securing configuration values and production credentials}

The maintenance of a back end server application often involve managing configuration values and secrets, such as encryption and API keys, and database access credentials. The naive and catastrophically insecure way of managing secrets is embedding them in the code, allowing attackers to harvest and exploit them~\cite{secretsinsourcecodes, meli2019bad}. Besides security issues, embedding secrets or infrastructure configuration in the code leads to a tight coupling between the infrastructure and the application, and it causes that the application needs to be redeployed in case of key rollovers or infrastructural changes. For mitigating security threats and tight coupling, I embedded neither configuration values, nor secrets into the application's code base — apart from the \textquote{dummy} secrets, which make local development easier. Instead, I use all such values referenced as environment variables.

In Azure, App Services' environment variables can be set on the service's graphical management interface, or their values can be provided by code-based application configuration\footnote{There are more advanced, more scalable solutions, such as the Azure App Configuration service~\cite{azure-app-config}, which I did not use and do not detail in this thesis.}. Providing environment variable values in code would be against the point, since that code would still be committed into the repository. Therefore I set all such environment variables on the App Service management interface.

App Services offer another level of secret abstraction by referencing Key Vault entries in environment variables. App Services substitute specially formatted reference strings found in environment variables with the actual value of the referenced Key Vault entry \emph{at runtime}. This way, sensitive secrets are not copy-pasted between services, and stored only one, centralized and audited, secure location.

When configuring the back end service, I placed insensitive configuration values (such as the database's name, URI and username) directly into environment variables, but sensitive secrets (such as the password and data encryption keys of the database, and the API key for SendGrid) are referenced values stored in the \emph{kv-prodcreds-prod-001} Key Vault.

\subsubsection{Creating multiple isolated back end environments}

I created two separated back end environments in order to be able to conduct safe testing, without compromising user data. The \emph{production} environment encompasses the back end application instances running on the production and staging deployment slots, storing data in the \emph{diplomatiqbackend-production} database. The \emph{development} environment comprises the instance running in the develop slot, and it stores data in the \emph{diplomatiqbackend-develop} database. Even though the two environments are not completely isolated from each other, as the two databases are stored on the same virtual machine, this way the \emph{development} environment can be regarded as a sandbox, which can not cause any harm to the \emph{production} environment. Storing configuration values separated from application code made supports the concept of multiple deployment environments well, since this way the exact same application can be deployed into both environments without needing compile-time reconfiguration.

\subsection{Setting up the database infrastructure}
\label{section:database}

\subsubsection{Overview}

Apart from Neo Technologies' own database service provider called Neo4j Aura~\cite{neo4j-aura}, no cloud service providers offer Neo4j in a database-as-a-service construction. For using Neo4j, I needed to purchase a virtual machine, install, configure and secure Neo4j Enterprise Edition, set up the databases, and perform the required network and access configuration across the back end service and the database machine.

\subsubsection{Procuring the virtual machine}

As I mentioned earlier, last months' high interest in cloud computing caused by the COVID-19 pandemic put a heavy load onto one of Azure's most popular regions, North Europe, where Diplomatiq's infrastructure is hosted. Microsoft introduced restrictions on new infrastructure deployments in the region, which prevented me from acquiring even a single instance of a small-sized virtual machine. Eventually the support allowed me to procure a virtual machine from one of the smallest pricing tiers, offering limited computing and IO performance only. This proved to be a serious bottleneck — the database answers slowly even to simply queries — but this was still a better solution than migrating Diplomatiq's entire infrastructure into another region.\footnote{Microsoft Azure does not support moving resources across regions for most of its resource types~\cite{azure-migration} The migration process would have meant recreating and reconfiguring the entire infrastructure in another region, migrating all data into the region, then deleting all resources in the old region.} After the restrictions are lifted, I will upscale the database instance into a higher pricing tier offering more performance.

\subsubsection{Installing, configuring and securing Neo4j Enterprise Edition}

The procured virtual machine came with the 18.04 version of Ubuntu operating system, as I requested. Having experience with securing Ubuntu servers, I set up and secured SSH-based authentication and disabled password-based logins, and I installed and configured \emph{ufw (Uncomplicated Firewall)} to let through only the ports 22 (for SSH), 7443 (for Neo4j HTTPS administration), and 7687 (for Neo4j binary communication).

I installed Neo4j Enterprise Edition, and configured its HTTPS connector to accept connections on the \emph{neo4j.diplomatiq.org} domain name. I created a public IP address for the virtual machine in Azure, and registered a DNS A record pointing the domain name to the IP address. For HTTPS connections, I also needed to acquire a valid TLS certificate issued to the domain name. For automated certificate management and renewal, I used the Electronic Frontier Foundation's \emph{certbot}~\cite{certbot}, which deploys free TLS certificates from Let's Encrypt, an open certificate authority~\cite{letsencrypt}. I also configured its Neo4j's Bolt\footnote{Bolt is Neo4j's binary communication protocol~\cite{boltprotocol}. The back end application communicates with the database over Bolt protocol.} connector to accept encrypted database connections only. I disabled the insecure HTTP connector.

On Neo4j's interactive web interface available on \emph{neo4j.diplomatiq.org}, I set up my own administrative user — with a long, cryptographically random password — to access the database, and deleted the default \emph{neo4j} user. Even though at this point both the database machine and Neo4j itself were properly secured, as an additional security measure, I sealed the machine off the internet with the help of Azure networking rules, making it available only within its own subnet, and another dedicated subnet.\footnote{The back end service operates within the other dedicated subnet.}\footnote{Neo4j's subnet can be reached from my personal IP address, for administrative purposes.}

\subsubsection{Setting up databases with dedicated users}

For the \emph{production} and the \emph{development} environments mentioned earlier, I set up two different graph databases within Neo4j, \emph{diplomatiqbackend-production} and \emph{diplomatiqbackend-develop}. I created a dedicated user for both databases, ensuring that the environments are separated along database credentials as well as all other configuration. Since Neo4j is a schemaless database, and indices and constraints can be automatically managed by Neo4j-OGM, the object-graph mapping framework I used at implementing the Diplomatiq application, there was no need for further configuration regarding the databases.

\subsection{Networking and security}

I placed all Diplomatiq server resources in a common virtual network to be able to apply firewalls, logging, and filtering rules to the infrastructure. I created the virtual network \emph{vnet-prod-northeurope-001} within the resource group \emph{rg-diplomatiq-prod-001}, and set its address space to \lstinline{10.0.0.0/16}.

Filtering network traffic can be achieved by security rules defined in \emph{Network Security Groups (NSGs)}. As NSGs are bound to subnets, at first I needed to create two subnets within \emph{vnet-prod-northeurope-001}:

\begin{itemize}
\item The \emph{snet-prod-northeurope-001} subnet contains Internet-facing services: the website, and the front end and back end services. Its address space is \lstinline{10.0.0.0/20}. As this subnet hosts only App Services, I delegated the whole subnet to the service \emph{Microsoft.Web/serverFarms} for more stable networking conditions~\cite{subnetdelegation}.
\item The \emph{snet-prod-northeurope-002} subnet contains resources that should not be available over the Internet: the database machine and the Key Vaults. Its address space is \lstinline{10.0.16.0/20}.
\end{itemize}

Then I created two NSGs: \emph{nsg-internetfacing-prod-001} and \emph{nsg-restrictive-prod-001}. The earlier allows any inbound traffic coming from the Internet on ports 80 and 443, and allows any outbound traffic. The latter allows inbound traffic only coming from the address space of \emph{snet-prod-northeurope-001} on ports 443 and 7687 — essentially it allows internal services to access the Key Vaults and Neo4j, but nothing else.

\Cref{fig:infrastructure} presents a summary about Diplomatiq's server infrastructure, displaying the main networking characteristics and rules.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{figures/infrastructure.pdf}
    \caption{Summary of Diplomatiq's server infrastructure}
    \label{fig:infrastructure}
\end{figure}

\subsection{Resource locking}

In order to prevent the accidental deletion of resources, I created a resource lock on the \emph{diplomatiq-prod-001} subscription. As long as this lock exists, the subscription and the contained resources are not allowed to be deleted.
